{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TnbzNWxIduJx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4fa43089-6e90-44a6-a867-37e5d9930125"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 5 rows:\n",
            "               date        p1        p2        p3        c1        c2  \\\n",
            "0  01-01-2019 01:00  0.859578  0.887445  0.958034 -0.782604 -1.257395   \n",
            "1  01-01-2019 02:00  0.862414  0.562139  0.781760 -1.940058 -1.872742   \n",
            "2  01-01-2019 03:00  0.766689  0.839444  0.109853 -1.207456 -1.277210   \n",
            "3  01-01-2019 04:00  0.976744  0.929381  0.362718 -1.027473 -1.938944   \n",
            "4  01-01-2019 05:00  0.455450  0.656947  0.820923 -1.125531 -1.845975   \n",
            "\n",
            "         c3 stability  \n",
            "0 -1.723086  unstable  \n",
            "1 -1.255012    stable  \n",
            "2 -0.920492  unstable  \n",
            "3 -0.997374  unstable  \n",
            "4 -0.554305  unstable  \n",
            "\n",
            "Last 5 rows:\n",
            "                   date        p1        p2        p3        c1        c2  \\\n",
            "43818  31-12-2023 19:00  0.257940  0.895296  0.868929 -1.954289 -0.981347   \n",
            "43819  31-12-2023 20:00  0.848075  0.909264  0.266201 -1.428185 -0.525543   \n",
            "43820  31-12-2023 21:00  0.393902  0.441923  0.697164 -0.701491 -1.279522   \n",
            "43821  31-12-2023 22:00  0.280877  0.532758  0.368188 -1.076426 -1.231602   \n",
            "43822  31-12-2023 23:00  0.237036  0.891977  0.656815 -1.579196 -1.526573   \n",
            "\n",
            "             c3 stability  \n",
            "43818 -1.654103    stable  \n",
            "43819 -1.545585    stable  \n",
            "43820 -1.019624  unstable  \n",
            "43821 -1.164986    stable  \n",
            "43822 -0.571834  unstable  \n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "\n",
        "# Load the CSV file\n",
        "file_path = '/content/final_merged_data_grid.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Print the first 5 rows\n",
        "print(\"First 5 rows:\")\n",
        "print(data.head())\n",
        "\n",
        "# Print the last 5 rows\n",
        "print(\"\\nLast 5 rows:\")\n",
        "print(data.tail())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "\n",
        "# Load the CSV file\n",
        "file_path = '/content/grid_stability_3months_validation_data.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Print the first 5 rows\n",
        "print(\"First 5 rows:\")\n",
        "print(data.head())\n",
        "\n",
        "# Print the last 5 rows\n",
        "print(\"\\nLast 5 rows:\")\n",
        "print(data.tail())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DkPrDsZl1rO-",
        "outputId": "91f272de-de1a-46e6-d413-da896c0c3f8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 5 rows:\n",
            "               date        c1        c2        c3        p1        p2  \\\n",
            "0  01-01-2024 00:00 -1.940140 -0.798906 -1.171796  0.209084  0.125430   \n",
            "1  01-01-2024 01:00 -1.431471 -0.523455 -1.481922  0.659985  0.565942   \n",
            "2  01-01-2024 02:00 -0.791358 -1.134176 -1.023553  0.083137  0.411446   \n",
            "3  01-01-2024 03:00 -1.291775 -1.943314 -1.536861  0.805780  0.431950   \n",
            "4  01-01-2024 04:00 -0.532246 -1.734586 -0.845011  0.067430  0.766504   \n",
            "\n",
            "         p3 stability  \n",
            "0  0.056619    stable  \n",
            "1  0.252764    stable  \n",
            "2  0.376176    stable  \n",
            "3  0.222319  unstable  \n",
            "4  0.349372    stable  \n",
            "\n",
            "Last 5 rows:\n",
            "                  date        c1        c2        c3        p1        p2  \\\n",
            "2179  31-03-2024 19:00 -1.593110 -1.096450 -1.287426  0.549650  0.107777   \n",
            "2180  31-03-2024 20:00 -1.987940 -1.172603 -1.136143  0.202879  0.613149   \n",
            "2181  31-03-2024 21:00 -0.974514 -1.665259 -1.477446  0.772918  0.565030   \n",
            "2182  31-03-2024 22:00 -0.787608 -1.179217 -0.716963  0.579977  0.898658   \n",
            "2183  31-03-2024 23:00 -1.399564 -0.925921 -1.463104  0.999409  0.526355   \n",
            "\n",
            "            p3 stability  \n",
            "2179  0.327430    stable  \n",
            "2180  0.487372  unstable  \n",
            "2181  0.422756    stable  \n",
            "2182  0.214563  unstable  \n",
            "2183  0.274401  unstable  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Path to the folder containing the CSV files\n",
        "folder_path = '/content/drive/My Drive/Grid_data'\n",
        "\n",
        "# List to hold DataFrames from each CSV file\n",
        "dfs = []\n",
        "\n",
        "# Iterate through each file in the folder\n",
        "for filename in os.listdir(folder_path):\n",
        "    if filename.endswith(\".csv\"):\n",
        "        # Read the CSV file into a DataFrame\n",
        "        year = filename.split('_')[2].split('.')[0]\n",
        "        df = pd.read_csv(os.path.join(folder_path, filename))\n",
        "        # Add a column indicating the year\n",
        "        df['year'] = year\n",
        "        dfs.append(df)\n",
        "\n",
        "# Concatenate all DataFrames into a single DataFrame\n",
        "combined_df = pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "# Sort the combined DataFrame by date\n",
        "combined_df.sort_values(by='date', inplace=True)\n",
        "\n",
        "# Display the sorted combined DataFrame\n",
        "print(combined_df)\n",
        "\n",
        "# Save the sorted combined DataFrame to a CSV file\n",
        "combined_file_path = '/content/drive/My Drive/Grid_data/combined_price_per_unit_sorted.csv'\n",
        "combined_df.to_csv(combined_file_path, index=False)\n"
      ],
      "metadata": {
        "id": "-ZUbkjl05XaH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the combined file with the year column\n",
        "combined_file_path = '/content/Grid_data/combined_price_per_unit_sorted.csv'\n",
        "combined_df = pd.read_csv(combined_file_path)\n",
        "\n",
        "# Display the DataFrame before deleting the 'year' column\n",
        "print(\"Before deleting 'year' column:\")\n",
        "print(combined_df)\n",
        "\n",
        "# Delete the 'year' column\n",
        "combined_df.drop(columns=['year'], inplace=True)\n",
        "\n",
        "# Display the DataFrame after deleting the 'year' column\n",
        "print(\"\\nAfter deleting 'year' column:\")\n",
        "print(combined_df)\n",
        "\n",
        "# Save the modified DataFrame to a new CSV file\n",
        "modified_combined_file_path = '/content/Grid_data/modified_combined_price_per_unit.csv'\n",
        "combined_df.to_csv(modified_combined_file_path, index=False)\n",
        "print(\"\\nModified DataFrame has been saved to:\", modified_combined_file_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pp1Ahw88e33G",
        "outputId": "26bae142-b3dd-43f6-d6e6-a66bcc0883ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before deleting 'year' column:\n",
            "                   date        p1        p2        p3  year\n",
            "0      01-01-2019 01:00  0.859578  0.887445  0.958034  unit\n",
            "1      01-01-2019 02:00  0.862414  0.562139  0.781760  unit\n",
            "2      01-01-2019 03:00  0.766689  0.839444  0.109853  unit\n",
            "3      01-01-2019 04:00  0.976744  0.929381  0.362718  unit\n",
            "4      01-01-2019 05:00  0.455450  0.656947  0.820923  unit\n",
            "...                 ...       ...       ...       ...   ...\n",
            "43818  31-12-2023 19:00  0.257940  0.895296  0.868929  unit\n",
            "43819  31-12-2023 20:00  0.848075  0.909264  0.266201  unit\n",
            "43820  31-12-2023 21:00  0.393902  0.441923  0.697164  unit\n",
            "43821  31-12-2023 22:00  0.280877  0.532758  0.368188  unit\n",
            "43822  31-12-2023 23:00  0.237036  0.891977  0.656815  unit\n",
            "\n",
            "[43823 rows x 5 columns]\n",
            "\n",
            "After deleting 'year' column:\n",
            "                   date        p1        p2        p3\n",
            "0      01-01-2019 01:00  0.859578  0.887445  0.958034\n",
            "1      01-01-2019 02:00  0.862414  0.562139  0.781760\n",
            "2      01-01-2019 03:00  0.766689  0.839444  0.109853\n",
            "3      01-01-2019 04:00  0.976744  0.929381  0.362718\n",
            "4      01-01-2019 05:00  0.455450  0.656947  0.820923\n",
            "...                 ...       ...       ...       ...\n",
            "43818  31-12-2023 19:00  0.257940  0.895296  0.868929\n",
            "43819  31-12-2023 20:00  0.848075  0.909264  0.266201\n",
            "43820  31-12-2023 21:00  0.393902  0.441923  0.697164\n",
            "43821  31-12-2023 22:00  0.280877  0.532758  0.368188\n",
            "43822  31-12-2023 23:00  0.237036  0.891977  0.656815\n",
            "\n",
            "[43823 rows x 4 columns]\n",
            "\n",
            "Modified DataFrame has been saved to: /content/Grid_data/modified_combined_price_per_unit.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Path to the folder containing the CSV files\n",
        "folder_path = '/content/Grid_data'\n",
        "\n",
        "# List to hold DataFrames from each CSV file\n",
        "dfs = []\n",
        "\n",
        "# Iterate through each file in the folder\n",
        "for filename in os.listdir(folder_path):\n",
        "    if filename.endswith(\".csv\"):\n",
        "        # Read the CSV file into a DataFrame\n",
        "        year = filename.split('_')[2].split('.')[0]\n",
        "        df = pd.read_csv(os.path.join(folder_path, filename))\n",
        "        # Add a column indicating the year\n",
        "        df['year'] = year\n",
        "        dfs.append(df)\n",
        "\n",
        "# Concatenate all DataFrames into a single DataFrame\n",
        "combined_df = pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "# Sort the combined DataFrame by date\n",
        "combined_df.sort_values(by='date', inplace=True)\n",
        "\n",
        "# Display the sorted combined DataFrame\n",
        "print(combined_df)\n",
        "\n",
        "# Save the sorted combined DataFrame to a CSV file\n",
        "combined_file_path = '/content/Grid_data/combined_unit_consumption_sorted.csv'\n",
        "combined_df.to_csv(combined_file_path, index=False)\n"
      ],
      "metadata": {
        "id": "YG96TMdYfQoW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e76dc1d0-0ec8-452b-ad07-becaf46a8cb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "                   date        c1        c2        c3  year\n",
            "8760   01-01-2019 01:00 -0.782604 -1.257395 -1.723086  2019\n",
            "8761   01-01-2019 02:00 -1.940058 -1.872742 -1.255012  2019\n",
            "8762   01-01-2019 03:00 -1.207456 -1.277210 -0.920492  2019\n",
            "8763   01-01-2019 04:00 -1.027473 -1.938944 -0.997374  2019\n",
            "8764   01-01-2019 05:00 -1.125531 -1.845975 -0.554305  2019\n",
            "...                 ...       ...       ...       ...   ...\n",
            "35034  31-12-2023 19:00 -1.954289 -0.981347 -1.654103  2023\n",
            "35035  31-12-2023 20:00 -1.428185 -0.525543 -1.545585  2023\n",
            "35036  31-12-2023 21:00 -0.701491 -1.279522 -1.019624  2023\n",
            "35037  31-12-2023 22:00 -1.076426 -1.231602 -1.164986  2023\n",
            "35038  31-12-2023 23:00 -1.579196 -1.526573 -0.571834  2023\n",
            "\n",
            "[43823 rows x 5 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the combined file with the year column\n",
        "combined_file_path = '/content/Grid_data/combined_unit_consumption_sorted.csv'\n",
        "combined_df = pd.read_csv(combined_file_path)\n",
        "\n",
        "# Display the DataFrame before deleting the 'year' column\n",
        "print(\"Before deleting 'year' column:\")\n",
        "print(combined_df)\n",
        "\n",
        "# Delete the 'year' column\n",
        "combined_df.drop(columns=['year'], inplace=True)\n",
        "\n",
        "# Display the DataFrame after deleting the 'year' column\n",
        "print(\"\\nAfter deleting 'year' column:\")\n",
        "print(combined_df)\n",
        "\n",
        "# Save the modified DataFrame to a new CSV file\n",
        "modified_combined_file_path = '/content/Grid_data/modified_combined_unit_consumption.csv'\n",
        "combined_df.to_csv(modified_combined_file_path, index=False)\n",
        "print(\"\\nModified DataFrame has been saved to:\", modified_combined_file_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G6hE044Y5e33",
        "outputId": "5fc5d2d8-93ad-4518-f5ce-c47e863ec489"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before deleting 'year' column:\n",
            "                   date        c1        c2        c3  year\n",
            "0      01-01-2019 01:00 -0.782604 -1.257395 -1.723086  2019\n",
            "1      01-01-2019 02:00 -1.940058 -1.872742 -1.255012  2019\n",
            "2      01-01-2019 03:00 -1.207456 -1.277210 -0.920492  2019\n",
            "3      01-01-2019 04:00 -1.027473 -1.938944 -0.997374  2019\n",
            "4      01-01-2019 05:00 -1.125531 -1.845975 -0.554305  2019\n",
            "...                 ...       ...       ...       ...   ...\n",
            "43818  31-12-2023 19:00 -1.954289 -0.981347 -1.654103  2023\n",
            "43819  31-12-2023 20:00 -1.428185 -0.525543 -1.545585  2023\n",
            "43820  31-12-2023 21:00 -0.701491 -1.279522 -1.019624  2023\n",
            "43821  31-12-2023 22:00 -1.076426 -1.231602 -1.164986  2023\n",
            "43822  31-12-2023 23:00 -1.579196 -1.526573 -0.571834  2023\n",
            "\n",
            "[43823 rows x 5 columns]\n",
            "\n",
            "After deleting 'year' column:\n",
            "                   date        c1        c2        c3\n",
            "0      01-01-2019 01:00 -0.782604 -1.257395 -1.723086\n",
            "1      01-01-2019 02:00 -1.940058 -1.872742 -1.255012\n",
            "2      01-01-2019 03:00 -1.207456 -1.277210 -0.920492\n",
            "3      01-01-2019 04:00 -1.027473 -1.938944 -0.997374\n",
            "4      01-01-2019 05:00 -1.125531 -1.845975 -0.554305\n",
            "...                 ...       ...       ...       ...\n",
            "43818  31-12-2023 19:00 -1.954289 -0.981347 -1.654103\n",
            "43819  31-12-2023 20:00 -1.428185 -0.525543 -1.545585\n",
            "43820  31-12-2023 21:00 -0.701491 -1.279522 -1.019624\n",
            "43821  31-12-2023 22:00 -1.076426 -1.231602 -1.164986\n",
            "43822  31-12-2023 23:00 -1.579196 -1.526573 -0.571834\n",
            "\n",
            "[43823 rows x 4 columns]\n",
            "\n",
            "Modified DataFrame has been saved to: /content/Grid_data/modified_combined_unit_consumption.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the files\n",
        "unit_consumption_file = '/content/modified_combined_unit_consumption.csv'\n",
        "price_per_unit_file = '/content/modified_combined_price_per_unit.csv'\n",
        "grid_stability_file = '/content/grid_stability_2019_2023.csv'\n",
        "\n",
        "unit_consumption_df = pd.read_csv(unit_consumption_file)\n",
        "price_per_unit_df = pd.read_csv(price_per_unit_file)\n",
        "grid_stability_df = pd.read_csv(grid_stability_file)\n",
        "\n",
        "# Merge unit_consumption_df with price_per_unit_df on 'date'\n",
        "merged_df = pd.merge(unit_consumption_df, price_per_unit_df, on='date', how='outer')\n",
        "\n",
        "# Merge merged_df with grid_stability_df on 'date'\n",
        "final_merged_df = pd.merge(merged_df, grid_stability_df, on='date', how='outer')\n",
        "\n",
        "# Display the final merged DataFrame\n",
        "print(final_merged_df)\n",
        "\n",
        "# Save the final merged DataFrame to a new CSV file\n",
        "final_merged_file_path = '/content/Grid_data/final_merged_data_2.csv'\n",
        "final_merged_df.to_csv(final_merged_file_path, index=False)\n",
        "print(\"\\nFinal merged DataFrame has been saved to:\", final_merged_file_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XEvFNfs755Ez",
        "outputId": "cfb70c94-c117-4fc7-8539-dae0dc8eebca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   date        c1        c2        c3        p1        p2  \\\n",
            "0      01-01-2019 01:00 -0.782604 -1.257395 -1.723086  0.859578  0.887445   \n",
            "1      01-01-2019 02:00 -1.940058 -1.872742 -1.255012  0.862414  0.562139   \n",
            "2      01-01-2019 03:00 -1.207456 -1.277210 -0.920492  0.766689  0.839444   \n",
            "3      01-01-2019 04:00 -1.027473 -1.938944 -0.997374  0.976744  0.929381   \n",
            "4      01-01-2019 05:00 -1.125531 -1.845975 -0.554305  0.455450  0.656947   \n",
            "...                 ...       ...       ...       ...       ...       ...   \n",
            "46002  31-03-2024 19:00       NaN       NaN       NaN       NaN       NaN   \n",
            "46003  31-03-2024 20:00       NaN       NaN       NaN       NaN       NaN   \n",
            "46004  31-03-2024 21:00       NaN       NaN       NaN       NaN       NaN   \n",
            "46005  31-03-2024 22:00       NaN       NaN       NaN       NaN       NaN   \n",
            "46006  31-03-2024 23:00       NaN       NaN       NaN       NaN       NaN   \n",
            "\n",
            "             p3 stability  \n",
            "0      0.958034  unstable  \n",
            "1      0.781760    stable  \n",
            "2      0.109853  unstable  \n",
            "3      0.362718  unstable  \n",
            "4      0.820923  unstable  \n",
            "...         ...       ...  \n",
            "46002       NaN    stable  \n",
            "46003       NaN  unstable  \n",
            "46004       NaN    stable  \n",
            "46005       NaN  unstable  \n",
            "46006       NaN  unstable  \n",
            "\n",
            "[46007 rows x 8 columns]\n",
            "\n",
            "Final merged DataFrame has been saved to: /content/Grid_data/final_merged_data_2.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "s10nqSIU8C3b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the final merged DataFrame\n",
        "final_merged_file_path = '/content/Grid_data/final_merged_data_2.csv'\n",
        "final_merged_df = pd.read_csv(final_merged_file_path)\n",
        "\n",
        "# Display the count of missing values in each column\n",
        "print(\"Count of missing values in each column:\")\n",
        "print(final_merged_df.isnull().sum())\n",
        "\n",
        "# Drop rows with any missing values\n",
        "final_merged_df.dropna(inplace=True)\n",
        "\n",
        "# Save the DataFrame after handling missing values to a new CSV file\n",
        "missing_values_removed_file_path = '/content/Grid_data/final_merged_data_missing_values_removed.csv'\n",
        "final_merged_df.to_csv(missing_values_removed_file_path, index=False)\n",
        "print(\"\\nDataFrame after handling missing values has been saved to:\", missing_values_removed_file_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U_3xy4mO628g",
        "outputId": "e3fef7c9-806a-47b9-f726-67592236f872"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Count of missing values in each column:\n",
            "date            0\n",
            "c1           2184\n",
            "c2           2184\n",
            "c3           2184\n",
            "p1           2184\n",
            "p2           2184\n",
            "p3           2184\n",
            "stability       0\n",
            "dtype: int64\n",
            "\n",
            "DataFrame after handling missing values has been saved to: /content/Grid_data/final_merged_data_missing_values_removed.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from scipy.stats import zscore\n",
        "\n",
        "# Load the final merged DataFrame\n",
        "final_merged_file_path = '/content/Grid_data/final_merged_data_missing_values_removed.csv'\n",
        "final_merged_df = pd.read_csv(final_merged_file_path)\n",
        "\n",
        "# Calculate Z-scores for numerical columns (excluding 'date' and 'stability' columns)\n",
        "numerical_cols = final_merged_df.select_dtypes(include=['float64', 'int64']).columns\n",
        "final_merged_df[numerical_cols] = final_merged_df[numerical_cols].apply(zscore)\n",
        "\n",
        "# Define the threshold for Z-score to identify outliers\n",
        "zscore_threshold = 3\n",
        "\n",
        "# Remove rows with Z-scores exceeding the threshold\n",
        "outlier_removed_df = final_merged_df[(final_merged_df[numerical_cols] < zscore_threshold).all(axis=1)]\n",
        "\n",
        "\n",
        "\n",
        "# Save the outlier removed DataFrame to a new CSV file\n",
        "outlier_removed_file_path = '/content/Grid_data/final_merged_data_outlier_removed.csv'\n",
        "outlier_removed_df.to_csv(outlier_removed_file_path, index=False)\n",
        "print(\"\\nOutliers removed DataFrame has been saved to:\", outlier_removed_file_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nmn-BzKA8Dvq",
        "outputId": "1a50c466-cff0-46f0-ccad-5480077c33a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Outliers removed DataFrame has been saved to: /content/Grid_data/final_merged_data_outlier_removed.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fiezpRBy8UD2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# Load the existing and newly prepared datasets\n",
        "unit_consumption_file = '/content/final_merged_data_outlier_removed.csv'\n",
        "validation_file = '/content/grid_stability_3months_validation_data.csv'\n",
        "\n",
        "unit_consumption_df = pd.read_csv(unit_consumption_file)\n",
        "validation_df = pd.read_csv(validation_file)\n",
        "\n",
        "# Prepare a database distributing power to three consumption nodes\n",
        "total_power = 44922.666632137916  # Total power generated\n",
        "node1_power = total_power * 0.20\n",
        "node2_power = total_power * 0.45\n",
        "node3_power = total_power * 0.35\n",
        "\n",
        "# Add power generated and stored at each node\n",
        "unit_consumption_df['p1'] = node1_power\n",
        "unit_consumption_df['p2'] = node2_power\n",
        "unit_consumption_df['p3'] = node3_power\n",
        "\n",
        "# Prepare features (X) and target variable (y)\n",
        "X = unit_consumption_df.drop(columns=['date', 'stability'])\n",
        "y = unit_consumption_df['stability']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Random Forest classifier\n",
        "clf = RandomForestClassifier(random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict stability on the testing set\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Calculate classification metrics\n",
        "classification_metrics = classification_report(y_test, y_pred)\n",
        "confusion_mat = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Print classification metrics\n",
        "print(\"Classification Metrics:\")\n",
        "print(classification_metrics)\n",
        "\n",
        "# Print confusion matrix\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_mat)\n",
        "\n",
        "# Validate the model using provided validation data\n",
        "X_validation = validation_df.drop(columns=['date', 'stability'])\n",
        "y_validation = validation_df['stability']\n",
        "\n",
        "# Predict stability on validation data\n",
        "y_validation_pred = clf.predict(X_validation)\n",
        "\n",
        "# Calculate F1 score, recall, and confusion matrix for validation data\n",
        "classification_metrics_validation = classification_report(y_validation, y_validation_pred)\n",
        "confusion_mat_validation = confusion_matrix(y_validation, y_validation_pred)\n",
        "\n",
        "# Print F1 score, recall, and confusion matrix for validation data\n",
        "print(\"\\nValidation Metrics:\")\n",
        "print(classification_metrics_validation)\n",
        "print(\"\\nConfusion Matrix (Validation Data):\")\n",
        "print(confusion_mat_validation)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2jJIX9Nr-QZ0",
        "outputId": "e61abd68-c266-4cb3-c68e-184e2e092fef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Metrics:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      stable       0.42      0.22      0.29      3187\n",
            "    unstable       0.65      0.83      0.73      5578\n",
            "\n",
            "    accuracy                           0.61      8765\n",
            "   macro avg       0.54      0.52      0.51      8765\n",
            "weighted avg       0.57      0.61      0.57      8765\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[ 709 2478]\n",
            " [ 965 4613]]\n",
            "\n",
            "Validation Metrics:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      stable       0.35      0.37      0.36       763\n",
            "    unstable       0.65      0.63      0.64      1421\n",
            "\n",
            "    accuracy                           0.54      2184\n",
            "   macro avg       0.50      0.50      0.50      2184\n",
            "weighted avg       0.55      0.54      0.54      2184\n",
            "\n",
            "\n",
            "Confusion Matrix (Validation Data):\n",
            "[[286 477]\n",
            " [529 892]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the existing dataset\n",
        "dataset_file = '/content/final_merged_data_outlier_removed.csv'\n",
        "dataset = pd.read_csv(dataset_file)\n",
        "\n",
        "# Calculate total power generated\n",
        "total_power = dataset[['p1', 'p2', 'p3']].sum(axis=1)\n",
        "\n",
        "# Calculate power stored at each node based on percentages\n",
        "node1_power = total_power * 0.20\n",
        "node2_power = total_power * 0.45\n",
        "node3_power = total_power * 0.35\n",
        "\n",
        "# Add power generated and stored at each node to the dataset\n",
        "dataset['total_power_generated'] = total_power\n",
        "dataset['node1_power'] = node1_power\n",
        "dataset['node2_power'] = node2_power\n",
        "dataset['node3_power'] = node3_power\n",
        "\n",
        "# Determine the percentage of 'Stable' and 'Unstable' grid conditions over the span of three months\n",
        "stable_count = dataset[dataset['stability'] == 'stable'].shape[0]\n",
        "unstable_count = dataset[dataset['stability'] == 'unstable'].shape[0]\n",
        "total_samples = dataset.shape[0]\n",
        "\n",
        "stable_percentage = (stable_count / total_samples) * 100\n",
        "unstable_percentage = (unstable_count / total_samples) * 100\n",
        "\n",
        "print(\"Percentage of Stable Grid Conditions:\", stable_percentage)\n",
        "print(\"Percentage of Unstable Grid Conditions:\", unstable_percentage)\n",
        "\n",
        "# Additional analysis can be performed to identify patterns in instability\n",
        "# For example, analyzing instability by hour of the day or day of the week\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LccDSokrAi6d",
        "outputId": "b4381e0d-9ada-4b59-adb4-20be08cba857"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Percentage of Stable Grid Conditions: 36.25265271661\n",
            "Percentage of Unstable Grid Conditions: 63.74734728339\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# Define a function to calculate metrics\n",
        "def calculate_metrics(df):\n",
        "    X = df.iloc[:, :-1]  # Select all columns except the last one (which is the target 'stability')\n",
        "    y = df.iloc[:, -1]   # Select the last column as the target variable 'stability'\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "    clf = RandomForestClassifier(random_state=42)\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    classification_metrics = classification_report(y_test, y_pred)\n",
        "    confusion_mat = confusion_matrix(y_test, y_pred)\n",
        "    return classification_metrics, confusion_mat\n",
        "\n",
        "# Calculate metrics for each node\n",
        "node1_metrics, node1_confusion_mat = calculate_metrics(node1_df)\n",
        "node2_metrics, node2_confusion_mat = calculate_metrics(node2_df)\n",
        "node3_metrics, node3_confusion_mat = calculate_metrics(node3_df)\n",
        "\n",
        "# Print nodewise performance\n",
        "print(\"Node 1 Performance Metrics:\")\n",
        "print(node1_metrics)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(node1_confusion_mat)\n",
        "\n",
        "print(\"\\nNode 2 Performance Metrics:\")\n",
        "print(node2_metrics)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(node2_confusion_mat)\n",
        "\n",
        "print(\"\\nNode 3 Performance Metrics:\")\n",
        "print(node3_metrics)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(node3_confusion_mat)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "z0kvBrH3B9Or",
        "outputId": "4d1c20f7-d24e-452d-afe1-11c028139eee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'node1_df' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-52d0d890376e>\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# Calculate metrics for each node\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mnode1_metrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode1_confusion_mat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode1_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0mnode2_metrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode2_confusion_mat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode2_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mnode3_metrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode3_confusion_mat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode3_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'node1_df' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# Load the existing and newly prepared datasets\n",
        "unit_consumption_file = '/content/final_merged_data_outlier_removed.csv'\n",
        "validation_file = '/content/Grid_data/grid_stability_3months_validation_data.csv'\n",
        "\n",
        "unit_consumption_df = pd.read_csv(unit_consumption_file)\n",
        "validation_df = pd.read_csv(validation_file)\n",
        "\n",
        "# Prepare a database distributing power to three consumption nodes\n",
        "total_power = 44922.666632137916  # Total power generated\n",
        "node1_power = total_power * 0.20\n",
        "node2_power = total_power * 0.45\n",
        "node3_power = total_power * 0.35\n",
        "\n",
        "# Add power generated and stored at each node\n",
        "unit_consumption_df['p1'] = node1_power\n",
        "unit_consumption_df['p2'] = node2_power\n",
        "unit_consumption_df['p3'] = node3_power\n",
        "\n",
        "# Prepare features (X) and target variable (y)\n",
        "X = unit_consumption_df.drop(columns=['date', 'stability'])\n",
        "y = unit_consumption_df['stability']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Random Forest classifier\n",
        "rf_clf = RandomForestClassifier(random_state=42)\n",
        "rf_clf.fit(X_train, y_train)\n",
        "\n",
        "# Train a Support Vector Machine classifier\n",
        "svm_clf = SVC(random_state=42)\n",
        "svm_clf.fit(X_train, y_train)\n",
        "\n",
        "# List of classifiers\n",
        "classifiers = {\n",
        "    \"Random Forest\": rf_clf,\n",
        "    \"Support Vector Machine\": svm_clf\n",
        "}\n",
        "\n",
        "# Iterate over classifiers\n",
        "for clf_name, clf in classifiers.items():\n",
        "    # Predict stability on the testing set\n",
        "    y_pred = clf.predict(X_test)\n",
        "\n",
        "    # Calculate classification metrics\n",
        "    classification_metrics = classification_report(y_test, y_pred)\n",
        "    confusion_mat = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "    # Print classifier name\n",
        "    print(f\"\\n{clf_name} Classifier Metrics:\")\n",
        "\n",
        "    # Print classification metrics\n",
        "    print(\"Classification Metrics:\")\n",
        "    print(classification_metrics)\n",
        "\n",
        "    # Print confusion matrix\n",
        "    print(\"\\nConfusion Matrix:\")\n",
        "    print(confusion_mat)\n",
        "\n",
        "    # Validate the model using provided validation data\n",
        "    X_validation = validation_df.drop(columns=['date', 'stability'])\n",
        "    y_validation = validation_df['stability']\n",
        "\n",
        "    # Predict stability on validation data\n",
        "    y_validation_pred = clf.predict(X_validation)\n",
        "\n",
        "    # Calculate F1 score, recall, and confusion matrix for validation data\n",
        "    classification_metrics_validation = classification_report(y_validation, y_validation_pred)\n",
        "    confusion_mat_validation = confusion_matrix(y_validation, y_validation_pred)\n",
        "\n",
        "    # Print F1 score, recall, and confusion matrix for validation data\n",
        "    print(\"\\nValidation Metrics:\")\n",
        "    print(classification_metrics_validation)\n",
        "    print(\"\\nConfusion Matrix (Validation Data):\")\n",
        "    print(confusion_mat_validation)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C_fim33rML-Z",
        "outputId": "26f67343-06c6-4ddf-be43-d04d15df6d12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Random Forest Classifier Metrics:\n",
            "Classification Metrics:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      stable       0.42      0.22      0.29      3187\n",
            "    unstable       0.65      0.83      0.73      5578\n",
            "\n",
            "    accuracy                           0.61      8765\n",
            "   macro avg       0.54      0.52      0.51      8765\n",
            "weighted avg       0.57      0.61      0.57      8765\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[ 709 2478]\n",
            " [ 965 4613]]\n",
            "\n",
            "Validation Metrics:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      stable       0.35      0.37      0.36       763\n",
            "    unstable       0.65      0.63      0.64      1421\n",
            "\n",
            "    accuracy                           0.54      2184\n",
            "   macro avg       0.50      0.50      0.50      2184\n",
            "weighted avg       0.55      0.54      0.54      2184\n",
            "\n",
            "\n",
            "Confusion Matrix (Validation Data):\n",
            "[[286 477]\n",
            " [529 892]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Support Vector Machine Classifier Metrics:\n",
            "Classification Metrics:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      stable       0.00      0.00      0.00      3187\n",
            "    unstable       0.64      1.00      0.78      5578\n",
            "\n",
            "    accuracy                           0.64      8765\n",
            "   macro avg       0.32      0.50      0.39      8765\n",
            "weighted avg       0.40      0.64      0.49      8765\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[   0 3187]\n",
            " [   0 5578]]\n",
            "\n",
            "Validation Metrics:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      stable       0.00      0.00      0.00       763\n",
            "    unstable       0.65      1.00      0.79      1421\n",
            "\n",
            "    accuracy                           0.65      2184\n",
            "   macro avg       0.33      0.50      0.39      2184\n",
            "weighted avg       0.42      0.65      0.51      2184\n",
            "\n",
            "\n",
            "Confusion Matrix (Validation Data):\n",
            "[[   0  763]\n",
            " [   0 1421]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# Load the existing and newly prepared datasets\n",
        "unit_consumption_file = '/content/final_merged_data_outlier_removed.csv'\n",
        "validation_file = '/content/grid_stability_3months_validation_data.csv'\n",
        "\n",
        "unit_consumption_df = pd.read_csv(unit_consumption_file)\n",
        "validation_df = pd.read_csv(validation_file)\n",
        "\n",
        "# Prepare a database distributing power to three consumption nodes\n",
        "total_power = 44922.666632137916  # Total power generated\n",
        "node1_power = total_power * 0.20\n",
        "node2_power = total_power * 0.45\n",
        "node3_power = total_power * 0.35\n",
        "\n",
        "# Add power generated and stored at each node\n",
        "unit_consumption_df['p1'] = node1_power\n",
        "unit_consumption_df['p2'] = node2_power\n",
        "unit_consumption_df['p3'] = node3_power\n",
        "\n",
        "# Prepare features (X) and target variable (y)\n",
        "X = unit_consumption_df.drop(columns=['date', 'stability'])\n",
        "y = unit_consumption_df['stability']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Random Forest classifier\n",
        "clf = RandomForestClassifier(random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict stability on the testing set\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Calculate classification metrics\n",
        "classification_metrics = classification_report(y_test, y_pred)\n",
        "confusion_mat = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Print classification metrics\n",
        "print(\"Classification Metrics:\")\n",
        "print(classification_metrics)\n",
        "\n",
        "# Print confusion matrix\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_mat)\n",
        "\n",
        "# Validate the model using provided validation data\n",
        "X_validation = validation_df.drop(columns=['date', 'stability'])\n",
        "y_validation = validation_df['stability']\n",
        "\n",
        "# Predict stability on validation data\n",
        "y_validation_pred = clf.predict(X_validation)\n",
        "\n",
        "# Calculate F1 score, recall, and confusion matrix for validation data\n",
        "classification_metrics_validation = classification_report(y_validation, y_validation_pred)\n",
        "confusion_mat_validation = confusion_matrix(y_validation, y_validation_pred)\n",
        "\n",
        "# Print F1 score, recall, and confusion matrix for validation data\n",
        "print(\"\\nValidation Metrics:\")\n",
        "print(classification_metrics_validation)\n",
        "print(\"\\nConfusion Matrix (Validation Data):\")\n",
        "print(confusion_mat_validation)\n",
        "\n",
        "# Include price per unit and unit consumption in the output\n",
        "print(\"\\nPrice per Unit and Unit Consumption:\")\n",
        "print(validation_df[['price_per_unit', 'unit_consumption']])\n",
        "\n",
        "# Include stability column in the output\n",
        "print(\"\\nActual Stability (Validation Data):\")\n",
        "print(validation_df['stability'])\n"
      ],
      "metadata": {
        "id": "qzQbgG2qHpdP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the CSV file\n",
        "file_path = 'final_merged_data_outlier_removed.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Display the head of the DataFrame\n",
        "print(\"Head of the DataFrame:\")\n",
        "print(data.head())\n",
        "\n",
        "# Display the tail of the DataFrame\n",
        "print(\"\\nTail of the DataFrame:\")\n",
        "print(data.tail())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H_Ch5ShxpKqt",
        "outputId": "7409fbb5-504b-4a3e-b683-c478755642df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Head of the DataFrame:\n",
            "               date        c1        c2        c3        p1        p2  \\\n",
            "0  01-01-2019 01:00  1.081419 -0.018211 -1.094476  1.222662  1.322361   \n",
            "1  01-01-2019 02:00 -1.593468 -1.438812 -0.012619  1.233007  0.135203   \n",
            "2  01-01-2019 03:00  0.099583 -0.063957  0.760554  0.883834  1.147188   \n",
            "3  01-01-2019 04:00  0.515523 -1.591649  0.582859  1.650043  1.475399   \n",
            "4  01-01-2019 05:00  0.288911 -1.377018  1.606921 -0.251455  0.481190   \n",
            "\n",
            "         p3 stability  \n",
            "0  1.580336  unstable  \n",
            "1  0.937256    stable  \n",
            "2 -1.513985  unstable  \n",
            "3 -0.591488  unstable  \n",
            "4  1.080132  unstable  \n",
            "\n",
            "Tail of the DataFrame:\n",
            "                   date        c1        c2        c3        p1        p2  \\\n",
            "43818  31-12-2023 19:00 -1.626354  0.619079 -0.935035 -0.971901  1.351014   \n",
            "43819  31-12-2023 20:00 -0.410524  1.671356 -0.684219  1.180705  1.401987   \n",
            "43820  31-12-2023 21:00  1.268871 -0.069295  0.531433 -0.475960 -0.303510   \n",
            "43821  31-12-2023 22:00  0.402394  0.041334  0.195457 -0.888237  0.027981   \n",
            "43822  31-12-2023 23:00 -0.759513 -0.639639  1.566406 -1.048153  1.338899   \n",
            "\n",
            "             p3 stability  \n",
            "43818  1.255263    stable  \n",
            "43819 -0.943600    stable  \n",
            "43820  0.628635  unstable  \n",
            "43821 -0.571533    stable  \n",
            "43822  0.481434  unstable  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: convert merged_data.xlsx to .csv\n",
        "\n",
        "!pip install openpyxl\n",
        "import pandas as pd\n",
        "\n",
        "# Read the Excel file\n",
        "df = pd.read_excel('merged_data.xlsx')\n",
        "\n",
        "# Save the DataFrame as a CSV file\n",
        "df.to_csv('merged_data.csv', index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vOKi8ad_y5Fb",
        "outputId": "e552e367-60c2-45e3-8ae5-9f5bb2278e7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.10/dist-packages (3.1.2)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.10/dist-packages (from openpyxl) (1.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the CSV file\n",
        "file_path = 'merged_data.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Display the head of the DataFrame\n",
        "print(\"Head of the DataFrame:\")\n",
        "print(data.head())\n",
        "\n",
        "# Display the tail of the DataFrame\n",
        "print(\"\\nTail of the DataFrame:\")\n",
        "print(data.tail())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rCPcHqHazPYi",
        "outputId": "8abdfddd-80c4-43ba-bb0d-294cbf36fead"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Head of the DataFrame:\n",
            "                  DateTime  Air temperature | (°C)  Pressure | (atm)  \\\n",
            "0  2019-01-01 01:00:00.000                  10.926          0.979103   \n",
            "1  2019-01-01 02:00:00.000                   9.919          0.979566   \n",
            "2  2019-01-01 03:00:00.005                   8.567          0.979937   \n",
            "3  2019-01-01 04:00:00.010                   7.877          0.980053   \n",
            "4  2019-01-01 05:00:00.015                   7.259          0.979867   \n",
            "\n",
            "   Power generated by system | (MW)  Wind speed | (m/s)  \n",
            "0                           33.6881               9.014  \n",
            "1                           37.2619               9.428  \n",
            "2                           30.5029               8.700  \n",
            "3                           28.4192               8.481  \n",
            "4                           27.3703               8.383  \n",
            "\n",
            "Tail of the DataFrame:\n",
            "                  DateTime  Air temperature | (°C)  Pressure | (atm)  \\\n",
            "43818  2023-12-31 19:00:00                  11.713          0.985015   \n",
            "43819  2023-12-31 20:00:00                  12.115          0.985244   \n",
            "43820  2023-12-31 21:00:00                  11.856          0.985639   \n",
            "43821  2023-12-31 22:00:00                  10.761          0.986212   \n",
            "43822  2023-12-31 23:00:00                   8.380          0.987183   \n",
            "\n",
            "       Power generated by system | (MW)  Wind speed | (m/s)  \n",
            "43818                           30.4221               8.703  \n",
            "43819                           32.0366               8.854  \n",
            "43820                           36.3990               9.333  \n",
            "43821                           37.7404               9.457  \n",
            "43822                           36.7908               9.324  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the first dataset with power distribution\n",
        "df_power_distribution = pd.read_excel(\"new_dataset_with_power_distribution.xlsx\")\n",
        "\n",
        "# Load the second dataset\n",
        "df_second_dataset = pd.read_csv(\"final_merged_data_outlier_removed.csv\")\n",
        "\n",
        "# Convert date/time columns to a consistent format\n",
        "df_power_distribution['DateTime'] = pd.to_datetime(df_power_distribution['DateTime'])\n",
        "df_second_dataset['date'] = pd.to_datetime(df_second_dataset['date'], format='%d-%m-%Y %H:%M')\n",
        "\n",
        "# Merge the datasets based on the date/time column and add only the required columns\n",
        "merged_df = pd.merge(df_second_dataset, df_power_distribution[['DateTime', 'g1', 'g2', 'g3']], left_on='date', right_on='DateTime', how='inner')\n",
        "\n",
        "# Drop the extra date/time column from the first dataset\n",
        "merged_df.drop(columns=['DateTime'], inplace=True)\n",
        "\n",
        "# Save the merged dataset\n",
        "merged_df.to_csv(\"final_merged_dataset.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "Og-jnvZ1pN4Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the merged dataset containing grid stability and power distribution\n",
        "df_merged = pd.read_csv(\"final_merged_dataset.csv\")\n",
        "\n",
        "# Now, you can analyze the stability of the power grid using the merged dataset\n",
        "\n",
        "# For example, you can calculate the percentage of stable and unstable grid conditions\n",
        "stability_percentage = df_merged['stability'].value_counts(normalize=True) * 100\n",
        "print(\"Percentage of Stable Grid Conditions:\", stability_percentage['stable'])\n",
        "print(\"Percentage of Unstable Grid Conditions:\", stability_percentage['unstable'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VAvaZ0WTzE8P",
        "outputId": "1c5496f4-4d46-4745-a0c0-8b9e362a06a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Percentage of Stable Grid Conditions: 36.249226325562205\n",
            "Percentage of Unstable Grid Conditions: 63.7507736744378\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the CSV file\n",
        "file_path = 'final_merged_dataset .csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Display the head of the DataFrame\n",
        "print(\"Head of the DataFrame:\")\n",
        "print(data.head())\n",
        "\n",
        "# Display the tail of the DataFrame\n",
        "print(\"\\nTail of the DataFrame:\")\n",
        "print(data.tail())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d1BPhlD03n71",
        "outputId": "0e52380e-0443-4c1e-c966-0681ae990219"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Head of the DataFrame:\n",
            "               date        c1        c2        c3        p1        p2  \\\n",
            "0  01-01-2019 01:00  1.081419 -0.018211 -1.094476  1.222662  1.322361   \n",
            "1  01-01-2019 02:00 -1.593468 -1.438812 -0.012619  1.233007  0.135203   \n",
            "2  01-01-2020 00:00 -0.454425  0.532479 -1.267142 -1.142116  1.341123   \n",
            "3  01-01-2020 01:00  1.154918 -0.366830  1.503881  0.653176  0.661482   \n",
            "4  01-01-2020 02:00 -0.361834  1.301290  1.622030  1.365003  0.227831   \n",
            "\n",
            "         p3        g1         g2         g3 stability  \n",
            "0  1.580336   6.73762  15.159645  11.790835  unstable  \n",
            "1  0.937256   7.45238  16.767855  13.041665    stable  \n",
            "2  0.454259   8.06582  18.148095  14.115185    stable  \n",
            "3  1.350370  11.41358  25.680555  19.973765    stable  \n",
            "4  1.562077  11.71690  26.363025  20.504575    stable  \n",
            "\n",
            "Tail of the DataFrame:\n",
            "                   date        c1        c2        c3        p1        p2  \\\n",
            "43618  31-12-2023 19:00 -1.626354  0.619079 -0.935035 -0.971901  1.351014   \n",
            "43619  31-12-2023 20:00 -0.410524  1.671356 -0.684219  1.180705  1.401987   \n",
            "43620  31-12-2023 21:00  1.268871 -0.069295  0.531433 -0.475960 -0.303510   \n",
            "43621  31-12-2023 22:00  0.402394  0.041334  0.195457 -0.888237  0.027981   \n",
            "43622  31-12-2023 23:00 -0.759513 -0.639639  1.566406 -1.048153  1.338899   \n",
            "\n",
            "             p3       g1         g2         g3 stability  \n",
            "43618  1.255263  6.08442  13.689945  10.647735    stable  \n",
            "43619 -0.943600  6.40732  14.416470  11.212810    stable  \n",
            "43620  0.628635  7.27980  16.379550  12.739650  unstable  \n",
            "43621 -0.571533  7.54808  16.983180  13.209140    stable  \n",
            "43622  0.481434  7.35816  16.555860  12.876780  unstable  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the 'date' column to datetime format\n",
        "date_format = \"%d-%m-%Y %H:%M\"\n",
        "df['date'] = pd.to_datetime(df['date'], format=date_format)\n",
        "\n",
        "# Filter the DataFrame to include only the last three months\n",
        "today = datetime(2023, 12, 31)  # Assuming the data is up to 31-12-2023\n",
        "three_months_ago = today - timedelta(days=90)\n",
        "last_three_months = (df['date'] >= pd.Timestamp(three_months_ago)) & (df['date'] <= pd.Timestamp(today))\n",
        "df_last_three_months = df.loc[last_three_months]\n",
        "\n",
        "# Calculate the percentage of 'Stable' and 'Unstable' grid conditions\n",
        "total_conditions = len(df_last_three_months)\n",
        "stable_conditions = len(df_last_three_months[df_last_three_months['stability'] == 'stable'])\n",
        "unstable_conditions = len(df_last_three_months[df_last_three_months['stability'] == 'unstable'])\n",
        "\n",
        "stable_percentage = (stable_conditions / total_conditions) * 100\n",
        "unstable_percentage = (unstable_conditions / total_conditions) * 100\n",
        "\n",
        "print(f\"Percentage of 'Stable' grid conditions: {stable_percentage:.2f}%\")\n",
        "print(f\"Percentage of 'Unstable' grid conditions: {unstable_percentage:.2f}%\")\n",
        "\n",
        "# Analyze the distribution of 'Unstable' conditions across different hours\n",
        "unstable_by_hour = df_last_three_months[df_last_three_months['stability'] == 'unstable'].groupby(df_last_three_months['date'].dt.hour).size()\n",
        "print(\"\\nDistribution of 'Unstable' conditions by hour:\")\n",
        "print(unstable_by_hour)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IyHVq_Um4T1-",
        "outputId": "86e46e3d-7966-4573-d229-edbf28c2fc3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Percentage of 'Stable' grid conditions: 37.11%\n",
            "Percentage of 'Unstable' grid conditions: 62.89%\n",
            "\n",
            "Distribution of 'Unstable' conditions by hour:\n",
            "date\n",
            "0     57\n",
            "1     62\n",
            "2     59\n",
            "3     64\n",
            "4     49\n",
            "5     53\n",
            "6     55\n",
            "7     54\n",
            "8     62\n",
            "9     54\n",
            "10    57\n",
            "11    53\n",
            "12    50\n",
            "13    68\n",
            "14    60\n",
            "15    57\n",
            "16    59\n",
            "17    58\n",
            "18    52\n",
            "19    57\n",
            "20    56\n",
            "21    60\n",
            "22    56\n",
            "23    47\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "f-BfPBVlN2Ie"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}